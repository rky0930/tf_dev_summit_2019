# TensorFlow Dev Summit 2019

## Day 1 - March 6th
<div class="devsite-table-wrapper"><table class="devsite-events-agenda android-dev-summit__agenda-table">
  <tbody>
    <tr style="border: none">
      <td><b>Time</b></td>
      <td><b>Subject (YouTube vidoe link)</b></td>
      <td><b>Speaker</b></td>
    </tr>
    <tr>
      <td>9:30 AM</td>
      <td><a href="https://youtu.be/b5Rs1ToD9aI">Keynote</a></td>
      <td>
        <b>Megan Kacholia</b>, Engineering Director<br>
        <b>Rajat Monga</b>, Engineering Director<br>
        <b>Kemal El Moujahid</b>, Director, Product Management<br>
        <b>Alina Shinkarsky</b>, Program Manager
      </td>
    </tr>
    <tr>
      <td>9:50 AM</td>
      <td><a href="https://youtu.be/k5c-vg4rjBw">TensorFlow 2.0</td>
      <td>
        <b>Karmel Allison</b>, Engineering Manager<br>
        <b>Martin Wicke</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>10:15 AM</td>
      <td><a href="https://youtu.be/DKosV_-4pdQ">TensorFlow Lite</a></td>
      <td>
        <b>Pete Warden</b>, Software Engineer<br>
        <b>Raziel Alvarez</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>10:55 AM</td>
      <td colspan="2">Break</td>
    </tr>
    <tr>
      <td>12:00 PM</td>
      <td><a href="https://youtu.be/xM8sO33x_OU">New features in TensorBoard<a></td>
      <td>
        <b>Gal Oshri</b>, Product Manager
      </td>
    </tr>
    <tr>
      <td>12:10 PM</td>
      <td><a href="https://youtu.be/Up9CvRLIIIw">Building Models with tf.function &amp; tf.autograph<a></td>
      <td>
        <b>Alexandre Passos</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>12:22 PM</td>
      <td><a href="https://youtu.be/-nTe44WT0ZI">Datasets and Models</a></td>
      <td>
        <b>Ryan Sepassi</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>12:30 PM</td>
      <td><a href="https://youtu.be/s65BigoMV_I">Swift for TensorFlow: </br>Next-Generation Machine Learning Framework</a></td>
      <td>
        <b>Chris Lattner</b>, Distinguished Engineer<br>
        <b>Brennan Saeta</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>1:00 PM</td>
      <td><a href="https://youtu.be/e_Drpr-wfAA">TensorFlow Community<a></td>
      <td>
        <b>Edd Wilder-James</b>, Open Source Strategist
      </td>
    </tr>
    <tr>
      <td>1:15 PM</td>
      <td colspan="2">Lunch</td>
    </tr>
    <tr>
      <td>2:35 PM</td>
      <td><a href="https://youtu.be/A5wiwT1qFjc">TensorFlow Extended (TFX): An End-to-End ML Platform</a> </br>Time stamp: start ~ to 19:51</td>
      <td>
        <b>Clemens Mewald</b>, Product Manager
      </td>
    </tr>
    <tr>
      <td>2:55 PM</td>
      <td><a href="https://www.youtube.com/watch?v=A5wiwT1qFjc&feature=youtu.be&list=PLQY2H8rRoyvzoUYI26kHmKSJBedn3SQuB&t=1192">Data Preparation with TFX Data Validation</a> </br>Time stamp: 19:52 ~ end</td>
      <td>
        <b>Clemens Mewald</b>, Product Manager
      </td>
    </tr>
    <tr>
      <td>3:10 PM</td>
      <td><a href="https://youtu.be/0O201IQlkxc">TFX Model Validation and TensorFlow Serving</a></td>
      <td>
        <b>Christina Greer</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>3:25 PM</td>
      <td><a href="https://youtu.be/y_qUJIfkbPs">TensorFlow Hub</a></td>
      <td>
        <b>André Susano Pinto</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>3:40 PM</td>
      <td colspan="2">Break</td>
    </tr>
    <tr>
      <td>4:45 PM</td>
      <td><a href="https://youtu.be/BrwKURU-wpk">TensorFlow Probability</a></td>
      <td>
        <b>Josh Dillon</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>4:55 PM</td>
      <td><a href="https://youtu.be/-TTziY7EmUA">Reinforcement Learning in TensorFlow<a></td>
      <td>
        <b>Sergio Guadarrama</b>, Senior Software Engineer<br>
        <b>Eugene Brevdo</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>5:10 PM</td>
      <td><a href="https://youtu.be/x35pOvZBJk8">JavaScript: </br>Writing models and </br>deploying them in the browser and Node.js</a></td>
      <td>
        <b>Yannick Assogba</b>, Front End Software Engineer<br>
        <b>Nick Kreeger</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>5:30 PM</td>
      <td><a href="https://youtu.be/8khPUtwaVaw">UniRoma: In Codice Ratio<a></td>
      <td>
        <b>Elena Nieddu</b>, UniRoma
      </td>
    </tr>
  </tbody>
</table></div>

## Day 2 - March 7th
<div class="devsite-table-wrapper"><table class="devsite-events-agenda android-dev-summit__agenda-table">
  <tbody>
    <tr style="border: none">
      <td>9:50 AM</td>
      <td>Lightning Talks</td>
      <td>
        <b><a href="https://youtu.be/lNGegNreehI">Butterfly</a></b> - Nathan Silberman<br>
        <b><a href="https://youtu.be/n2MwJ1guGVQ">TensorFlow.jl (Julia)</a></b> - Jonathan Malmoud<br>
        <b><a href="https://youtu.be/Y8Nfcjg0faw">NetEase</a></b> - Huijie Lin<br>
        <b><a href="https://youtu.be/ABBnNjbjv2Q">TF Lattice</a></b> - Maya Gupta<br>
        <b>Alibaba</b> - Wei Lin<br>
        <b><a href="https://youtu.be/D1c2pi624X4">TF.text</a></b> - Mark Omernick<br>
        <b><a href="https://youtu.be/paJNSODuu3c">Uber Manifold</a></b> - Lezhi Li<br>
        <b><a href="https://youtu.be/GRMvCeIKvps">TF.js at Creative Labs</a></b> - Irene Alvarado<br>
      </td>
    </tr>
    <tr>
      <td>11:15 AM</td>
      <td>Breakout sessions</td>
      <td>
        <b>2.0 and Porting Models</b><br>
        Karmel Allison, Martin Wicke, Tomer Kaftan, Alex Passos, Anna Revinskaya<br>
        <br>
        <b>TensorFlow at Scale</b><br>
        Jiri Simsa, Reed Wanderman-Milne, Penporn Koanantakool, Yuefeng Zhou<br>
        <br>
        <b>TensorFlow On-Device: Compressing, Quantizing, and Distributing</b><br>
        YC Ling, Suharsh Sivakumar, Sara Sirajuddin, Tim Davis, Pete Warden
      </td>
    </tr>
    <tr>
      <td>12:00 PM</td>
      <td>Lunch</td>
      <td>
        <b>Contributors Luncheon</b> for those interested in SIGs and more<br>
        <br>
        <b>TensorFlow Extended (TFX)</b> workshop<br>
        <a href="https://github.com/rcrowe-google/TFX-DevSummit-2019">TFX Workshop  Meterials GitHub Repo</a>
      </td>
    </tr>
    <tr>
      <td>1:15 PM</td>
      <td>Research and the Future</td>
      <td>
        <b><a href="https://youtu.be/e0QK5glozC8">NERSC - Exascale Deep Learning for Climate Analytics</a></b><br>
        Thorsten Kurth<br>
        <br>
        <b><a href="https://youtu.be/1YbPmkChcbo">Federated Learning</a></b><br>
        Krzysztof Ostrowski<br>
        <br>
        <b><a href="https://youtu.be/HgGyWS40g-g">Mesh TensorFlow</a></b><br>
        Noam Shazeer<br>
        <br>
        <b><a href="https://youtu.be/rlpQjnUvoKw">Sonnet 2.0</a></b><br>
        Tamara Norman, Malcolm Reynolds
      </td>
    </tr>
  </tbody>
</table></div>


# Summaries in Text
### Keynote:
  - skip

### TensorFlow 2.0
  - TensorFlow 2.0 alpha realsed(since 2019/3/6)
    - pip install -U --pre tensorflow
  - What is changed: 
    - Usability:
      - tf.keras as the high-level API
      - Eager execution by default
    - Clarity:
      - Remove duplicate functionality
      - Consistant intuitive syntax across APIs
      - Compatibility throughout the TensorFlow ecosystem
    - Flexibility:
      - Full lower-level API
      - Internal ops accessible in tf.raw_ops
      - Inheritable interfaces for variables, checkpoints layers
  - How do i upgrade from 1.0 to 2.0? # "Google is now under process of converting one of the largest codebases in the world"
    - Provide migration guides and best practices
    - Provide Escape to backwords compatibility module: tf.compat.v1(contains all of the 1.x API except tf.contrib)
    - Provide conversion script: tf_upgrade_v2 # "Note that this automatic conversion it will fix it, so, it works, but it won't fix your style"
  - Timeline
    - Alpha
      - Available now
    - Next release candidate
      - In spring
      - Implementing some missing features
        - Converting libraries
        - Converting Google
      - A lots of testing and optimization
    - RC
      - Release testing
      - Integration testing
  - Progress
    - Check [Tensorflow 2.0 Project Tracker](https://github.com/orgs/tensorflow/projects/4)
  - Go build
    - pip installl -U --pre tensorflow 
    - Docs: tensorflow.org/r2.0
  - High Level APIs
    - Keras inside tensorflow: tf.keras
      - Standardizing on the keras API for building layers and models
      - Include all the power of estomators
      - You cam move from prototype to distributed training, to production serving in one go
      - In 2.0, 1.x keras model definition will run in Eager mode without any modification
      - tf.keras.optimizer.*
      - tf.keras.metrics.*
      - tf.keras.losses.*
      - tf.keras.layers.*
    - Eager mode
      - Easy to debug
      - Dynamic control
    - Unified RNN layers
      - In 2.0, there is on version of the LSTM and one version of GRU layer and they select the write operation for available devices for runtime
      - tf.keras provides an API that is easy to subclass and customize, so that user can innovate on top of the existing layers
      - User can use feature colum to parse data and feed it directly in to downstream keras layers and this feature column work both with Keras and Estimators. 
        - User can mix ans match to create reusable data input pipelines
    - Tensorboard
      - Tensorboard integration with Keras as a simple as one line
        - tf.keras.callbacks.TensorBoard(log_dir=log_dir) and Use the return value as a param of model.fit(...)
        - Performance profilling is built-in
    - Going big: tf.distribute.Strategy
      - Use Multi-GPU in simple code and Scaling efficiency is greater than 90% over multiple GPUs
    - To SavedModel and beyond
      - Easily export models for use with TF Serving, TF Lite and more
    - Comming soon: 
      - Multi-node, Multi-worker synchronous training
      - Distributing tf.keras with ParmeterServerStrategy
      - Exposing canned Estimators from the Keras API
      - Handling Very Large Models with variable partitioning
      - ... and much more ! 

### Tensorflow LITE
  - Why Tensorflow Lite ? On-device ML allows building new types of products !
    - Access to more data
    - Fast and closely knit interactions
    - Privacy preserving
  - Challenges
    - Reduced compute power
    - Limited memory
    - Battery constraints
  - Simplifying ML on-device
    - Tensorflow Lite makes these challenges much easier
  - Use cases > 2B mobile devices (Have Tensorflow Lite deployed on them in production)
    - Text - Classification, Prediction
    - Speach - Recognition, Text to Speach, Speach to Text
    - Image - Object detection, Object location, OCR, Gesture Recognition, Facial modeling, Segmentation, Clustering, Compression, Super Resolution
    - Audio - Translation, Voice Synthesis
    - Content - Voice generation, Text generation, Audio generation
  - Use case 1 - Google assistant
    - 1B + devices
      - Wide ragne of devices: High/low end, arm, x86, battery powered, plugged in, many operating systems
    - Key Speach On-Device Capabilites
      - "Hey Google" Hotword with VoiceMatch
        - Tiny memory and computation footprint running continuously
        - Extremely latency sensitive
      - On-device speech recognition
        - High computation running in shorter bursts
    - Why Did We Migrate to TFLite?
      - TensorFlow Lite meets or beats our existing libraries size and speed
      - Tensorflow Lite lays the groundwork to accelerate our models on GPUs, Edge TPUs, etc.
  - Use case 2 - NetEase youdao
    - Company introduction
      - Online Education Brand with the largest numbers of users in China (800 million Users, 22 million DAU)
    - Youdao Application with TensorFlow Lite
      - App introduction
        - Youdao Dictionary - Most popular dictionary App in China 
        - Youdao Translator - Most populary translation App in China
        - U-Dictionary - Most popular dictionary and language learning App in India
      - Detail App features
        - Conveniently look up words 
          - Camera recognize words from the images and do OCR and the translation on your devices
          - Use Tensorflow Lite to accelerate the OCR and translation services
        - Photo translation 
          - Scenarios:
            - Use camera to take a photo from the many scenarios and it will do the whole image OCR ans translate the text into another languages
            - Erase the text on the image and replace the origianl text to the translated text 
            - Replace the original text to the translated text
            - Present to users with the translations
          - Use TensorFlow Lite to accelerate the OCR and translation services
    - Why NetEase youdao choose TensorFlow Lite ?
      - OCR and Translation are very sensitive to the binary size and also computation resources and it responding time
      - To accelerate abillites on device 
      - To provide the more efficient on device inferences
  - TensorFlow Lite themes
    - Usability - Get your models up and running
      - Model conversion 
        - Steps
          - TensorFlow -> Savede Model -> TF Lite Converter -> TF Lite Model
        - failures & solutions
          - Limited ops => Tensorflow Select
            - In the pipeline(future work): TensorFlow Select + Selective Registration
          - Unsupported semantics (e.g. control-flow in RNNs) => Not yet
            - In the pipeline(future work): Control flow support (e.g. loops conditions)
        - Tensorflow Select
          - Available now 
            - Enables hundresd more ops from TensorFlow on CPU
            - Caveat: binary size increase(~6MB compressed)     
          - In the pipeline(future work)
            - Selective registration
              - Selective registration is already something user can take adavangate of TensorFlow Lite for our building ops. So, user only include in the binary the ops that user is really using. So, user don't end up increasing user's binary size unnecessarily
            - Improved performance
              - Trying to blur the of the TensorFlow ops and the Tensorflow Lite ops are. One of the key points of this is blur the performance gap that there might be between one and the other  <--- blur ?? blow ??
        - Control flow support
          - In the pipeline(future work)
            - Control flow are core to many ops(e.g RNNs) and graphs. Thus TensorFlow Lite team is adding support for:
              - Loops
              - Conditions
        - Converter 2.0
          - Follow users feedback! TensorFlow Lite team is building a new converter that will answer:
            - What went wrong ?
            - Where it went wrong ?
            - How can i fix it ?        
    - Performance - Get your models executing as fast as possible
      - Incrediable inference performance
        - CPU: Pixel2 - Single threadded CPU
        - Model: MobileNet v1
        - Performance: 
          - CPU: 124 ms
          - CPU(Quantization): 64 ms
          - GPU(Flow OpenGL 16): 16 ms
          - Edge TPU(Quatized Fixed point): 2 ms
      - Benchmarking
        - Model Benchmark tool
          - Benchmarking and profiling
            - Available
              - Support for threading
              - Per op profiling
              - Support for Android NN API
      - Acceleration by delegate
        - Edge TPU delegate
          - High performance
          - Small physical and power footprint
        - GPU delegate
          - Preview available!
            - 2-7x faster than the floating point CPU implementation
            - Adds ~250KB to binary size(Android/iOS)
            - Simple to add GPU Support in source code(just need to add few lines)
          - In the pipeline(future works)
            - Expand coverage of operations
            - Further optimize performance
            - Evolve and finalize the APIs
        - Android Neural Network API(NNAPI) delete
          - Enables hardware supported by the Android NN API
        - CPU optimazations
          - TensorFlow Lite team knows CPU deployments are important!
          - In the pipeline
            - Further optimizations on these architectures:
              - Arm
              - x86
    - Optimization - Make your models even smaller and faster
      - Quatization
        - Available 
          - Post-training quatization (CPU) - recommaned to use
            - Post-training quantization with float & fixed point
            - Great for CPU deployments!
            - Benefits:
              - 4x reduction in model sizes
              - models, which consist primarily of convolutional layers get 10~50% faster execution (CPU)
              - Fully-connected & RNN-based models get up to 3x speed-up (CPU)
            - Extremely simple to try
            - User have to validate how the accuray is affected in user's particular model. But so far, TensorFlow Lite team have seen very good numbers
            - Many of the 2B devices that above mentioned are running this way
        - In the pipeline(future work) - Even better performance on CPU, Plus enable many NPUs!
          - Keras-based quantized training (CPU/NPU)
            - Training with quantization Keras-based API
          - Post-training quantization (CPU/NPU)
            - Post-training quantization with fixed point math only
        - Quantization Steps (post-training)
          - Tensorflow(estimator or Keras) > Saved Model + Calibration Data > TF Lite Converter > TF Lite Model  
        - Quantization results: training vs post-training - Result is almost the same
          - Top 1 accuracy
            - Model | float baseline | Quantization during training | Quantization after training
            - Mobilenet v1 | 70.95% | 69.97% | 69.54%
            - Resnet v2 ___| 76.8% _| 76.7% _| 76.6%
            - Mobilenet v1 | 77.9% _| 77.5% _| 77.7%
      - Other optimizations
        - Available
          - Model optimization toolkit
        - In the pipeline(future work)
          - Keras-based connection pruning
            - Connection pruning
              - What does is mean?
                - Drop connections during training
                - Dense tensors will now be sparse (filled with zeros)
              - Benefits
                - Smaller models - Sparse tensors can be compressed
                - Faster models - Less operations to execute
            - Coming soon
              - Training with connection pruning in Keras-based API (compression benefits)
            - In the pipeline(future work)
              - Inference support for sparse models (speed-ups on CPU and selected NPUs)
            - Simple to add pruning in source code(just need to add few lines)
        - Pruning results
          - Negligible accuracy loss at 50% sparsity
          - Small accuracy loss at 75%
    - Documentation - Resources to get the most out of TensorFlow Lite
      - Docs
        - Available
          - New tensorflow.org/lite
            - Revamped TensorFlow Lite documentation with new tutorials & guides
            - Published TensorFlow Lite future roadmap
        - In the pipeplie(future work)
          - More tutorials
      - Model repository
        - Available
          - In depth sample application & tutorials for(5 models):
              - Image classification
              - Object detection
              - Pose estimation
              - Segmentation
              - Smart reply
        - In the pipeplie(future work)
          - Expand repository to many more models
    - TF Mobile Deprecated
      - Provided 6+ monthes of notice
      - Limiting developer support in favor of TensorFlow Lite
      - Still available for training on Github
    - What about Traing on device using TensorFlow Lite ?
      - Training 
        - Building many of the pieces
          - TF Select + Control Flow + Subgraphs + Variables
            - Gradient calculation
            - Forward Backward
            - Initialization
            - Storage during training
        - Nothing to announce now, but, TensorFlow Lite team working on it and thinking a lot about it
    - What about Tensorflow 2.0 ?
      - TensorFlow Lite support TensorFlow 2.0
        - SavedModel -> Tensorflow Lite
    - Before we go.. 2 last thing !
      - 1) Pretty Cool New Project by Pete Warden - MCU
        - Tiny models on tiny computers !
          - Microcontroller are everywhere
          - Speech researchers were pioneers
          - Models just tens of kilobytes
        - Why the models had to be so small ? 
          - To save battery !
          - Microcontrollers use far less power than CPUs
          - Main CPU is turned off while MCU listens
          - Only tens of kilobytes of RAM and Flash available
          - No cloud connection
        - Pete thought this approach would be widely useful
          - Open up voice interfaces to more developers
          - Work with other kinds of noisy sensor data
        - Google releasing the first experimental support for embedded platforms in tensorflow light
          - Pocket size demonstration board is showed up !
          - A prototype of a development board built by Sparkfun [link](https://www.sparkfun.com/products/15170)
            - 32-bit ARM Cortex-M4F processor with Direct Memory Access
            - 48MHz CPU clock, 96MHz with TurboSPOT™
            - Extremely low-power usage: 6uA/MHz
            - 1MB Flash
            - 384KB SRAM
            - Dedicated Bluetooth processor with BLE 5
            - Available to run on a single coin battery for many days ! 
          - Live Demo by Pete Warden
            - Say "Yes"
            - Yellow light blink
            - [Demo Video Link](https://youtu.be/DKosV_-4pdQ?t=1951)
        - Why is this useful ?
          - Running entirely on-device
          - Tiny constraints
            - It's using a 20 KB model
            - Run using less than 100KB of RAM and 80KB of Flash
          - All Open Source & Train your own model
            - https://aiyprojects.withgoogle.com/open_speech_recording
            - https://www.tensorflow.org/tutorials/sequences/audio_recognition
          - Try it for yourself!
            - https://www.sparkfun.com/products/15170
            - Designed to be portable, runs on many other platform
            - Looking forward to collaborating ! 
      - 2) Teachable Machine - On-Device Transfer Learning on the Coral Dev Board
        - What is Coral ?
          - Coral is a platform for creating products with on-device ML acceleration
          - First products features Google's Edge TPU in SBC and USB accessory forms
        - Edge TPU
          - A Google-designed AISC that lets you run inference on-device:
            - Very fast inference speed(Object detection in less than 15ms)
            - Enables greater data privacy
            - No reliance on a network connection
            - Runs inference with Tensorflow Lite
          - Enables unique workloads and new applications Like on-device real-time transfer learning
          
        - Coral Products
          - Coral Dev Board
            - A single-board computer with a removable system-on-module (SOM) featuring the Edge TPU.
            - Supported OS: Mendel Linux (derivative of Debian), Android
            - Supported Framework: TensorFlow Lite
            - Languages: Python (C++ coming soon)
            - [Datasheet](https://coral.withgoogle.com/tutorials/devboard-datasheet/)
            - [Learn More](https://coral.withgoogle.com/products/dev-board/)
          - Coral Accelerator
            - A USB accessory featuring the Edge TPU that brings ML inferencing to existing systems.
            - Supported OS: Debian Linux
            - Compatible with Raspberry Pi boards
            - Supported Framework: TensorFlow Lite
            - [Datasheet](https://coral.withgoogle.com/tutorials/accelerator-datasheet/)
            - [Learn More](https://coral.withgoogle.com/products/accelerator/)
          - Edge Training Demo
            - Three ways to do edge training:
              - k-nearest neighbors
              - Weight imprinting
              - Last layer retraining
            - Teachable Machine 
              - K-nearest neighbors approach is used
              - Live Demo by June
                - Teachable Machine showed up
                  - Edge TPU development board assembled with a camera and series of buttons
                  - Each botton corresponds with the class and lights up when the model identifes an objects from the camera
                  - The training process is done on edge device immediately
                  - Classify perfectly
                  - [Demo Video Link](https://youtu.be/DKosV_-4pdQ?t=2527)

### TensorBoard
    - What's new with TensorBoard ?
      - Summary
        - TensorBoard in Colab and Jupyter Notebook
        - Easier comparison of train and validation runs
        - Keras conceptual graph visualization
        - Hyperparameter tuning with the HParams dashboard
      - Details
        - "Tensor Board Team enables showing tensor board directly within colab and Jupyter Notebook"
          - How ?
            - Train with model.fit() and give it a tensorboard callback
            - This logged the right data to visualize in Tensor Board
          - Before, User have to download the logs to local machine to visualize the data with Tensor Board
        - "Train and Validation showing up on the same charts to make it much easier to compare them in accuracy loss"
        - More flexable Graph Dashboard
        - Several APIs for using Tensor Board within notebooks that user change the height of the cell as well as list the active instances within user's collab notebook
        - Hyperparameter Tuning
          - Available in the TF 2.0 alpha
          - A couple of hyper-parameters
            - Dropout rate, the number of units, the dense layer and optimizer
          - How to use ?
            - Several additional import
            - Define which values of the hyperparameters user want to try
              - Ex) num_units_list = [16, 32]
              - dropout_rate_list = [0.1, 0.2]
              - optimizer_list = ['adam']
            - Define matrix
              - ex) accuracy
            - Wrap the existing training code
            - Start Training
            - The progress of traing is showed within Tensor Board
            - Go to "HPARAMS" dashboard
              - TABLE VIEW
                - Each run is represented by a row
                - Each columns for each of the hyper parameters
                - Filtering and Sorting are available in the matrix
              - PARALLEL COORDINATES VIEW
              - SCATTER PLOT MATRIX VIEW
