# tf_dev_summit_2019

## Day 1 - March 6th
<div class="devsite-table-wrapper"><table class="devsite-events-agenda android-dev-summit__agenda-table">
  <tbody>
    <tr style="border: none">
      <td><b>Time</b></td>
      <td><b>Subject(YouTube vidoe link)</b></td>
      <td><b>Speaker</b></td>
    </tr>
    <tr>
      <td>9:30 AM</td>
      <td><a href="https://youtu.be/b5Rs1ToD9aI">Keynote</a></td>
      <td>
        <b>Megan Kacholia</b>, Engineering Director<br>
        <b>Rajat Monga</b>, Engineering Director<br>
        <b>Kemal El Moujahid</b>, Director, Product Management<br>
        <b>Alina Shinkarsky</b>, Program Manager
      </td>
    </tr>
    <tr>
      <td>9:50 AM</td>
      <td><a href="https://youtu.be/k5c-vg4rjBw">TensorFlow 2.0</td>
      <td>
        <b>Karmel Allison</b>, Engineering Manager<br>
        <b>Martin Wicke</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>10:15 AM</td>
      <td><a href="https://youtu.be/DKosV_-4pdQ">TensorFlow Lite</a></td>
      <td>
        <b>Pete Warden</b>, Software Engineer<br>
        <b>Raziel Alvarez</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>10:55 AM</td>
      <td colspan="2">Break</td>
    </tr>
    <tr>
      <td>12:00 PM</td>
      <td><a href="https://youtu.be/xM8sO33x_OU">New features in TensorBoard<a></td>
      <td>
        <b>Gal Oshri</b>, Product Manager
      </td>
    </tr>
    <tr>
      <td>12:10 PM</td>
      <td><a href="https://youtu.be/Up9CvRLIIIw">Building Models with tf.function &amp; tf.autograph<a></td>
      <td>
        <b>Alexandre Passos</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>12:22 PM</td>
      <td><a href="https://youtu.be/-nTe44WT0ZI">Datasets and Models</a></td>
      <td>
        <b>Ryan Sepassi</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>12:30 PM</td>
      <td><a href="https://youtu.be/s65BigoMV_I">Swift for TensorFlow: </br>Next-Generation Machine Learning Framework</a></td>
      <td>
        <b>Chris Lattner</b>, Distinguished Engineer<br>
        <b>Brennan Saeta</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>1:00 PM</td>
      <td><a href="https://youtu.be/e_Drpr-wfAA">TensorFlow Community<a></td>
      <td>
        <b>Edd Wilder-James</b>, Open Source Strategist
      </td>
    </tr>
    <tr>
      <td>1:15 PM</td>
      <td colspan="2">Lunch</td>
    </tr>
    <tr>
      <td>2:35 PM</td>
      <td><a href="https://youtu.be/A5wiwT1qFjc">TensorFlow Extended (TFX): An End-to-End ML Platform</a> </br>Time stamp: start ~ to 19:51</td>
      <td>
        <b>Clemens Mewald</b>, Product Manager
      </td>
    </tr>
    <tr>
      <td>2:55 PM</td>
      <td><a href="https://www.youtube.com/watch?v=A5wiwT1qFjc&feature=youtu.be&list=PLQY2H8rRoyvzoUYI26kHmKSJBedn3SQuB&t=1192">Data Preparation with TFX Data Validation</a> </br>Time stamp: 19:52 ~ end</td>
      <td>
        <b>Clemens Mewald</b>, Product Manager
      </td>
    </tr>
    <tr>
      <td>3:10 PM</td>
      <td><a href="https://youtu.be/0O201IQlkxc">TFX Model Validation and TensorFlow Serving</a></td>
      <td>
        <b>Christina Greer</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>3:25 PM</td>
      <td><a href="https://youtu.be/y_qUJIfkbPs">TensorFlow Hub</a></td>
      <td>
        <b>Andr√© Susano Pinto</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>3:40 PM</td>
      <td colspan="2">Break</td>
    </tr>
    <tr>
      <td>4:45 PM</td>
      <td><a href="https://youtu.be/BrwKURU-wpk">TensorFlow Probability</a></td>
      <td>
        <b>Josh Dillon</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>4:55 PM</td>
      <td><a href="https://youtu.be/-TTziY7EmUA">Reinforcement Learning in TensorFlow<a></td>
      <td>
        <b>Sergio Guadarrama</b>, Senior Software Engineer<br>
        <b>Eugene Brevdo</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>5:10 PM</td>
      <td><a href="https://youtu.be/x35pOvZBJk8">JavaScript: </br>Writing models and </br>deploying them in the browser and Node.js</a></td>
      <td>
        <b>Yannick Assogba</b>, Front End Software Engineer<br>
        <b>Nick Kreeger</b>, Software Engineer
      </td>
    </tr>
    <tr>
      <td>5:30 PM</td>
      <td><a href="https://youtu.be/8khPUtwaVaw">UniRoma: In Codice Ratio<a></td>
      <td>
        <b>Elena Nieddu</b>, UniRoma
      </td>
    </tr>
  </tbody>
</table></div>

## Day 2 - March 7th
<div class="devsite-table-wrapper"><table class="devsite-events-agenda android-dev-summit__agenda-table">
  <tbody>
    <tr style="border: none">
      <td>9:50 AM</td>
      <td>Lightning Talks</td>
      <td>
        <b><a href="https://youtu.be/lNGegNreehI">Butterfly</a></b> - Nathan Silberman<br>
        <b><a href="https://youtu.be/n2MwJ1guGVQ">TensorFlow.jl (Julia)</a></b> - Jonathan Malmoud<br>
        <b><a href="https://youtu.be/Y8Nfcjg0faw">NetEase</a></b> - Huijie Lin<br>
        <b><a href="https://youtu.be/ABBnNjbjv2Q">TF Lattice</a></b> - Maya Gupta<br>
        <b>Alibaba</b> - Wei Lin<br>
        <b><a href="https://youtu.be/D1c2pi624X4">TF.text</a></b> - Mark Omernick<br>
        <b><a href="https://youtu.be/paJNSODuu3c">Uber Manifold</a></b> - Lezhi Li<br>
        <b><a href="https://youtu.be/GRMvCeIKvps">TF.js at Creative Labs</a></b> - Irene Alvarado<br>
      </td>
    </tr>
    <tr>
      <td>11:15 AM</td>
      <td>Breakout sessions</td>
      <td>
        <b>2.0 and Porting Models</b><br>
        Karmel Allison, Martin Wicke, Tomer Kaftan, Alex Passos, Anna Revinskaya<br>
        <br>
        <b>TensorFlow at Scale</b><br>
        Jiri Simsa, Reed Wanderman-Milne, Penporn Koanantakool, Yuefeng Zhou<br>
        <br>
        <b>TensorFlow On-Device: Compressing, Quantizing, and Distributing</b><br>
        YC Ling, Suharsh Sivakumar, Sara Sirajuddin, Tim Davis, Pete Warden
      </td>
    </tr>
    <tr>
      <td>12:00 PM</td>
      <td>Lunch</td>
      <td>
        <b>Contributors Luncheon</b> for those interested in SIGs and more<br>
        <br>
        <b>TensorFlow Extended (TFX)</b> workshop
      </td>
    </tr>
    <tr>
      <td>1:15 PM</td>
      <td>Research and the Future</td>
      <td>
        <b><a href="https://youtu.be/e0QK5glozC8">NERSC</a></b><br>
        Thorsten Kurth<br>
        <br>
        <b><a href="https://youtu.be/1YbPmkChcbo">Federated Learning</a></b><br>
        Krzysztof Ostrowski<br>
        <br>
        <b><a href="https://youtu.be/HgGyWS40g-g">Mesh TensorFlow</a></b><br>
        Noam Shazeer<br>
        <br>
        <b><a href="https://youtu.be/rlpQjnUvoKw">Sonnet 2.0</a></b><br>
        Tamara Norman, Malcolm Reynolds
      </td>
    </tr>
  </tbody>
</table></div>


# Summaries in Text
### Keynote:
  - skip

### TensorFlow 2.0
  - TensorFlow 2.0 alpha realsed(since 2019/3/6)
    - pip install -U --pre tensorflow
  - What is changed: 
    - Usability:
      - tf.keras as the high-level API
      - Eager execution by default
    - Clarity:
      - Remove duplicate functionality
      - Consistant intuitive syntax across APIs
      - Compatibility throughout the TensorFlow ecosystem
    - Flexibility:
      - Full lower-level API
      - Internal ops accessible in tf.raw_ops
      - Inheritable interfaces for variables, checkpoints layers
  - How do i upgrade from 1.0 to 2.0? # "Google is now under process of converting one of the largest codebases in the world"
    - Provide migration guides and best practices
    - Provide Escape to backwords compatibility module: tf.compat.v1(contains all of the 1.x API except tf.contrib)
    - Provide conversion script: tf_upgrade_v2 # "Note that this automatic conversion it will fix it, so, it works, but it won't fix your style"
  - Timeline
    - Alpha
      - Available now
    - Next release candidate
      - In spring
      - Implementing some missing features
        - Converting libraries
        - Converting Google
      - A lots of testing and optimization
    - RC
      - Release testing
      - Integration testing
  - Progress
    - Github.com > Tensorflow > Tensorflow_2.0 > Project tracker
  - Go build
    - pip installl -U --pre tensorflow 
    - Docs: tensorflow.org/r2.0
  - High Level APIs
    - Keras inside tensorflow: tf.keras
      - Standardizing on the keras API for building layers and models
      - Include all the power of estomators
      - You cam move from prototype to distributed training, to production serving in one go
      - In 2.0, 1.x keras model definition will run in Eager mode without any modification
      - tf.keras.optimizer.*
      - tf.keras.metrics.*
      - tf.keras.losses.*
      - tf.keras.layers.*
    - Eager mode
      - Easy to debug
      - Dynamic control
    - Unified RNN layers
      - In 2.0, there is on version of the LSTM and one version of GRU layer and they select the write operation for available devices for runtime
      - tf.keras provides an API that is easy to subclass and customize, so that user can innovate on top of the existing layers
      - User can use feature colum to parse data and feed it directly in to downstream keras layers and this feature column work both with Keras and Estimators. 
        - User can mix ans match to create reusable data input pipelines
    - Tensorboard
      - Tensorboard integration with Keras as a simple as one line
        - tf.keras.callbacks.TensorBoard(log_dir=log_dir) and Use the return value as a param of model.fit(...)
        - Performance profilling is built-in
    - Going big: tf.distribute.Strategy
      - Use Multi-GPU in simple code and Scaling efficiency is greater than 90% over multiple GPUs
    - To SavedModel and beyond
      - Easily export models for use with TF Serving, TF Lite and more
    - Comming soon: 
      - Multi-node, Multi-worker synchronous training
      - Distributing tf.keras with ParmeterServerStrategy
      - Exposing canned Estimators from the Keras API
      - Handling Very Large Models with variable partitioning
      - ... and much more ! 

### Tensorflow LITE
  - Why Tensorflow Lite ? On-device ML allows building new types of products !
    - Access to more data
    - Fast and closely knit interactions
    - Privacy preserving
  - Challenges
    - Reduced compute power
    - Limited memory
    - Battery constraints
  - Simplifying ML on-device
    - Tensorflow Lite makes these challenges much easier
  - Use cases > 2B mobile devices (Have Tensorflow Lite deployed on them in production)
    - Text - Classification, Prediction
    - Speach - Recognition, Text to Speach, Speach to Text
    - Image - Object detection, Object location, OCR, Gesture Recognition, Facial modeling, Segmentation, Clustering, Compression, Super Resolution
    - Audio - Translation, Voice Synthesis
    - Content - Voice generation, Text generation, Audio generation
  - Use case 1 - Google assistant
    - 1B + devices
      - Wide ragne of devices: High/low end, arm, x86, battery powered, plugged in, many operating systems
    - Key Speach On-Device Capabilites
      - "Hey Google" Hotword with VoiceMatch
        - Tiny memory and computation footprint running continuously
        - Extremely latency sensitive
      - On-device speech recognition
        - High computation running in shorter bursts
    - Why Did We Migrate to TFLite?
      - TensorFlow Lite meets or beats our existing libraries size and speed
      - Tensorflow Lite lays the groundwork to accelerate our models on GPUs, Edge TPUs, etc.
  - Use case 2 - NetEase youdao
    - Company introduction
      - Online Education Brand with the largest numbers of users in China (800 million Users, 22 million DAU)
    - Youdao Application with TensorFlow Lite
      - App introduction
        - Youdao Dictionary - Most popular dictionary App in China 
        - Youdao Translator - Most populary translation App in China
        - U-Dictionary - Most popular dictionary and language learning App in India
      - Detail App features
        - Conveniently look up words 
          - Camera recognize words from the images and do OCR and the translation on your devices
          - Use Tensorflow Lite to accelerate the OCR and translation services
        - Photo translation 
          - Scenarios:
            - Use camera to take a photo from the many scenarios and it will do the whole image OCR ans translate the text into another languages
            - Erase the text on the image and replace the origianl text to the translated text 
            - Replace the original text to the translated text
            - Present to users with the translations
          - Use TensorFlow Lite to accelerate the OCR and translation services
    - Why NetEase youdao choose TensorFlow Lite ?
      - OCR and Translation are very sensitive to the binary size and also computation resources and it responding time
      - To accelerate abillites on device 
      - To provide the more efficient on device inferences
  - TensorFlow Lite themes
    - Usability - Get your models up and running
      - Model conversion 
        - Steps
          - TensorFlow > Savede Model > TF Lite Converter > TF Lite Model
        - failures & solutions
          - Limited ops => Tensorflow Select
            - In the pipeline(future work): TensorFlow Select + Selective Registration
          - Unsupported semantics (e.g. control-flow in RNNs) => Not yet
            - In the pipeline(future work): Control flow support (e.g. loops conditions)
        - Tensorflow Select
          - Available now 
            - Enables hundresd more ops from TensorFlow on CPU
            - Caveat: binary size increase(~6MB compressed)     
          - In the pipeline(future work)
            - Selective registration
              - Selective registration is already something user can take adavangate of TensorFlow Lite for our building ops. So, user only include in the binary the ops that user is really using. So, user don't end up increasing user's binary size unnecessarily
            - Improved performance
              - Trying to blur the of the TensorFlow ops and the Tensorflow Lite ops are. One of the key points of this is blur the performance gap that there might be between one and the other  <--- blur ?? blow ??
        - Control flow support
          - In the pipeline(future work)
            - Control flow are core to many ops(e.g RNNs) and graphs. Thus TensorFlow Lite team is adding support for:
              - Loops
              - Conditions
        - Converter 2.0
          - Follow users feedback! TensorFlow Lite team is building a new converter that will answer:
            - What went wrong ?
            - Where it went wrong ?
            - How can i fix it ?        
    - Performance - Get your models executing as fast as possible
      - Incrediable inference performance
        - CPU: Pixel2 - Single threadded CPU
        - Model: MobileNet v1
        - Performance: 
          - CPU: 124 ms
          - CPU(Quantization): 64 ms
          - GPU(Flow OpenGL 16): 16 ms
          - Edge TPU(Quatized Fixed point): 2 ms
      - Benchmarking
        - Model Benchmark tool
          - Benchmarking and profiling
            - Available
              - Support for threading
              - Per op profiling
              - Support for Android NN API
      - What is a delegate ?
        - [Ï∂îÍ∞ÄÌïòÍ∏∞]
      - Acceleration by delegate
        - Edge TPU delegate
          - High performance
          - Small physical and power footprint
        - GPU delegate
          - Preview available!
            - 2-7x faster than the floating point CPU implementation
            - Adds ~250KB to binary size(Android/iOS)
            - Simple to add GPU Support in source code(just need to add few lines)
          - In the pipeline(future works)
            - Expand coverage of operations
            - Further optimize performance
            - Evolve and finalize the APIs
        - Android Neural Network API(NNAPI) delete
          - Enables hardware supported by the Android NN API
        - CPU optimazations
          - TensorFlow Lite team knows CPU deployments are important!
          - In the pipeline
            - Further optimizations on these architectures:
              - Arm
              - x86
    - Optimization - Make your models even smaller and faster
      - Quatization
        - Available 
          - Post-training quatization (CPU) - recommaned to use
            - Post-training quantization with float & fixed point
            - Great for CPU deployments!
            - Benefits:
              - 4x reduction in model sizes
              - models, which consist primarily of convolutional layers get 10~50% faster execution (CPU)
              - Fully-connected & RNN-based models get up to 3x speed-up (CPU)
            - Extremely simple to try
            - User have to validate how the accuray is affected in user's particular model. But so far, TensorFlow Lite team have seen very good numbers
            - Many of the 2B devices that above mentioned are running this way
        - In the pipeline(future work) - Even better performance on CPU, Plus enable many NPUs!
          - Keras-based quantized training (CPU/NPU)
            - Training with quantization Keras-based API
          - Post-training quantization (CPU/NPU)
            - Post-training quantization with fixed point math only
        - Quantization Steps (post-training)
          - Tensorflow(estimator or Keras) > Saved Model + Calibration Data > TF Lite Converter > TF Lite Model  
        - Quantization results: training vs post-training - Result is almost the same
          - Top 1 accuracy
            - Model | float baseline | Quantization during training | Quantization after training
            - Mobilenet v1 | 70.95% | 69.97% | 69.54%
            - Resnet v2 ___| 76.8% _| 76.7% _| 76.6%
            - Mobilenet v1 | 77.9% _| 77.5% _| 77.7%
      - Other optimizations
        - Available
          - Model optimization toolkit
        - In the pipeline(future work)
          - Keras-based connection pruning
            - Connection pruning
              - What does is mean?
                - Drop connections during training
                - Dense tensors will now be sparse (filled with zeros)
              - Benefits
                - Smaller models - Sparse tensors can be compressed
                - Faster models - Less operations to execute
            - Coming soon
              - Training with connection pruning in Keras-based API (compression benefits)
            - In the pipeline(future work)
              - Inference support for sparse models (speed-ups on CPU and selected NPUs)
            - Simple to add pruning in source code(just need to add few lines)
        - Pruning results
          - Negligible accuracy loss at 50% sparsity
          - Small accuracy loss at 75%
    - Documentation - Resources to get the most out of TensorFlow Lite
      - Docs
        - Available
          - New tensorflow.org/lite
            - Revamped TensorFlow Lite documentation with new tutorials & guides
            - Published TensorFlow Lite future roadmap
        - In the pipeplie(future work)
          - More tutorials
      - Model repository
        - Available
          - In depth sample application & tutorials for(5 models):
              - Image classification
              - Object detection
              - Pose estimation
              - Segmentation
              - Smart reply
        - In the pipeplie(future work)
          - Expand repository to many more models
    - TF Mobile Deprecated
      - Provided 6+ monthes of notice
      - Limiting developer support in favor of TensorFlow Lite
      - Still available for training on Github
    - What about Traing on device using TensorFlow Lite ?
      - Training 
        - Building many of the pieces
          - TF Select + Control Flow + Subgraphs + Variables
            - Gradient calculation
            - Forward Backward
            - Initialization
            - Storage during training
        - Nothing to announce now, but, TensorFlow Lite team working on it and thinking a lot about it
    - What about Tensorflow 2.0 ?
      - TensorFlow Lite support TensorFlow 2.0
        - SavedModel -> Tensorflow Lite
    - Before we go.. 2 last thing !
      - 